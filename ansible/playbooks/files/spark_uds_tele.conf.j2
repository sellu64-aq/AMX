[global_tags]
  project = "{{ project }}"
  zone = "{{ zone }}"
  nodeIP = "{{ ansible_host }}"
  service = "spark"
  stream = "uds"

[agent]
  debug = true
  quiet = false
  interval = "30s"
  flush_interval = "30s"
  logfile = "{{ telegraf_remote_path }}/telegraf/logs/{{ telegraf_name }}_telegraf.log"
  hostname = "{{ inventory_hostname }}"
  omit_hostname = false
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 20000
  collection_jitter = "0s"
  flush_jitter = "0s"
  precision = "0s"
  logfile_rotation_interval = "12h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 5

#############################################################################
[[inputs.prometheus]]
 urls = ["http://{{ ansible_host }}:{{ spark_port }}/metrics"]
 name_override = "spark_raw"
 metric_version = 2
 interval = "50s"
 timeout = "30s"

###############################################################################
# PROCESSOR-1: CATEGORIZE AND NORMALIZE SPARK METRICS (GENERIC)
###############################################################################
[[processors.starlark]]
  namepass = ["spark_raw"]
  source = '''
def apply(metric):
    new_fields = {}
    new_tags = {}

    for key, value in metric.fields.items():
        # ================================================================
        # JVM METRICS
        # ================================================================
        if key.startswith("jvm_") or key.startswith("java_lang_"):
            metric.name = "spark_jvm"
            new_fields[key] = value

        # ================================================================
        # KAFKA CONSUMER METRICS
        # ================================================================
        elif key.startswith("kafka_consumer_"):
            metric.name = "spark_kafka"
            new_fields[key] = value

        # ================================================================
        # SPARK STREAMING METRICS - GENERIC EXTRACTION
        # ================================================================
        elif key.startswith("metrics_local_"):
            parts = key.split("_")

            # Strip metrics_local_<id>_driver_ prefix if present
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                stripped = "_".join(parts[4:])
            else:
                stripped = key

            # Check if this is a streaming metric
            if "streamingmetrics_streaming_" in stripped:
                metric.name = "spark_streaming"

                # Extract service name (everything before "streamingmetrics_streaming_")
                service_name = extract_service_name_generic(stripped)
                if service_name:
                    new_tags["streaming_service"] = service_name

                # Extract standard metric name (everything after "streamingmetrics_streaming_")
                standard_metric = extract_standard_metric(stripped)
                if standard_metric:
                    new_fields[standard_metric] = value

            # LiveListenerBus metrics
            elif "livelistenerbus" in stripped:
                metric.name = "spark_livelistenerbus"
                new_fields[stripped] = value

            # Code Generator metrics
            elif "codegenerator" in stripped:
                metric.name = "spark_codegenerator"
                new_fields[stripped] = value

            # Executor metrics
            elif "executor" in stripped:
                metric.name = "spark_executor"
                new_fields[stripped] = value

            # Block Manager metrics
            elif "blockmanager" in stripped:
                metric.name = "spark_blockmanager"
                new_fields[stripped] = value

            # JVM Pool metrics
            elif "jvm_pools" in stripped:
                metric.name = "spark_jvm_pools"
                new_fields[stripped] = value

            # Hive Catalog metrics
            elif "hiveexternalcatalog" in stripped:
                metric.name = "spark_hive"
                new_fields[stripped] = value

            # DAG Scheduler metrics
            elif "dagscheduler" in stripped:
                metric.name = "spark_dagscheduler"
                new_fields[stripped] = value

            # Other Spark metrics
            else:
                metric.name = "spark_other"
                new_fields[stripped] = value

        # ================================================================
        # MISCELLANEOUS METRICS
        # ================================================================
        else:
            metric.name = "spark_misc"
            new_fields[key] = value

    # Clear and update fields
    metric.fields.clear()
    for k, v in new_fields.items():
        metric.fields[k] = v

    # Add new tags
    for k, v in new_tags.items():
        metric.tags[k] = v

    return metric

# ================================================================
# HELPER FUNCTION: Extract service name generically
# ================================================================
def extract_service_name_generic(field_name):
    """
    Extract service name by taking everything BEFORE "streamingmetrics_streaming_"

    Examples:
      gmsa_orchastration_engine_notify_limit_streamingmetrics_streaming_...
        → gmsa_orchastration_engine_notify_limit

      orchestrationstreamingapi_gmsa_addorder_streamingmetrics_streaming_...
        → orchestrationstreamingapi_gmsa_addorder

      projectx_some_service_name_streamingmetrics_streaming_...
        → projectx_some_service_name
    """
    if "streamingmetrics_streaming_" in field_name:
        # Split on the marker and take everything before it
        service_name = field_name.split("streamingmetrics_streaming_")[0]
        # Remove trailing underscore if present
        service_name = service_name.rstrip("_")
        return service_name

    return "unknown"

# ================================================================
# HELPER FUNCTION: Extract standard metric name
# ================================================================
def extract_standard_metric(field_name):
    """
    Extract the standard metric name by taking everything AFTER "streamingmetrics_streaming_"
    This is the common suffix that all streaming services share.

    Examples:
      gmsa_orchastration_engine_notify_limit_streamingmetrics_streaming_lastcompletedbatch_processingdelay_value
        → streaming_lastcompletedbatch_processingdelay_value

      orchestrationstreamingapi_gmsa_addorder_streamingmetrics_streaming_waitingbatches_value
        → streaming_waitingbatches_value
    """
    if "streamingmetrics_streaming_" in field_name:
        # Split on the marker and take everything after it
        # Keep "streaming_" prefix for clarity
        standard_part = field_name.split("streamingmetrics_streaming_")[1]
        return "streaming_" + standard_part

    return field_name
'''

###############################################################################
# PROCESSOR-2: REMOVE NaN AND INFINITE VALUES
###############################################################################
[[processors.starlark]]
  namepass = ["spark_*"]
  source = '''
def apply(metric):
    remove = []
    for k, v in metric.fields.items():
        # Remove NaN and Inf values
        if str(v) == "nan" or v == float("inf") or v == float("-inf"):
            remove.append(k)

    # Remove invalid fields
    for k in remove:
        metric.fields.pop(k)

    return metric
'''

###############################################################################
# PROCESSOR-3: ADD COMPUTED FIELDS (OPTIONAL)
###############################################################################
[[processors.starlark]]
  namepass = ["spark_streaming"]
  source = '''
def apply(metric):
    """
    Add computed fields for better monitoring:
    - batch_efficiency: processingdelay / totaldelay
    - is_lagging: 1 if waitingbatches > 0 else 0
    - processing_rate: records per millisecond
    """
    fields = metric.fields

    # Calculate batch processing efficiency
    processing_delay = fields.get("streaming_lastcompletedbatch_processingdelay_value", 0)
    total_delay = fields.get("streaming_lastcompletedbatch_totaldelay_value", 1)

    if total_delay > 0:
        fields["batch_efficiency_percent"] = (processing_delay / total_delay) * 100

    # Add lag indicator
    waiting_batches = fields.get("streaming_waitingbatches_value", 0)
    fields["is_lagging"] = 1 if waiting_batches > 0 else 0

    # Add batch throughput (records / processing time)
    total_records = fields.get("streaming_totalprocessedrecords_value", 0)
    total_batches = fields.get("streaming_totalcompletedbatches_value", 1)

    if total_batches > 0:
        fields["avg_records_per_batch"] = total_records / total_batches

    # Calculate processing rate (if we have timing data)
    last_batch_records = fields.get("streaming_lastreceivedbatch_records_value", 0)
    if processing_delay > 0 and last_batch_records > 0:
        fields["records_per_second"] = (last_batch_records / processing_delay) * 1000

    return metric
'''


########## Streaming Logs #### --> Sample Logs below #####
#2026-01-27 12:23:00 :210 - Record Count : 1
#2026-01-27 12:23:00 :213 -======================================  Start Orchastration Processing=================================
#2026-01-27 12:23:01 :120 -Max number of api_sequence: 1
#2026-01-27 12:23:03 :34 -[Before getConnection] HikariCP Pool Stats → Total: 1, Active: 0, Idle: 1, Waiting Threads: 0
#2026-01-27 12:23:03 :40 -Connection acquired.
#2026-01-27 12:23:03 :34 -[After getConnection] HikariCP Pool Stats → Total: 1, Active: 1, Idle: 0, Waiting Threads: 0
#2026-01-27 12:23:03 :224 -Tue Jan 27 12:23:03 UTC 2026:73344074182033559:====Initiate API Calling Process:[http://cmp-prod-goup-gmsa:8080/GOUPRouter/rout/downloadProfileEsim]====
#12:23:05 :34 -[After getConnection] HikariCP Pool Stats → Total: 10, Active: 1, Idle: 9, Waiting Threads: 0
#2026-01-27 12:23:05 :614 -Truncated table: param_table_7
#2026-01-27 12:23:05 :219 -======================================End Orchastration Processing=================================

[[inputs.tail]]
  ## Orchestration notification log file
  files = ["{{ stream_log_path }}/logging.log"]

  ## Read from current position
  initial_read_offset = "beginning"

  ## File watch method
  watch_method = "inotify"

  ## Data parsing format
  data_format = "grok"

  ## Measurement name
  name_override = "spark_orchestration_logs"

  ## Grok patterns for orchestration logs
  grok_patterns = [
    # Pattern 1: Date Time :LineNumber - Message
    "^%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day} %{TIME:time} :%{NUMBER:line_number} -\\s*%{GREEDYDATA:message}$",

    # Pattern 2: Date Time :LineNumber -RequestID:ICCID - Message
    "^%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day} %{TIME:time} :%{NUMBER:line_number} -%{DATA:request_id},ICCID is:\\[%{DATA:iccid}\\]:%{GREEDYDATA:message}$",

    # Pattern 3: Date Time :LineNumber -RequestID - Message
    "^%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day} %{TIME:time} :%{NUMBER:line_number} -%{DATA:request_id}:%{GREEDYDATA:message}$"
  ]

  ## Timezone
  grok_timezone = "UTC"

  ## Additional tags
  [inputs.tail.tags]
    application = "spark"
    servicename = "streaming_service"
    component = "api_orchestrator"
    log_type = "orchestration"

## Convert fields to tags
[[processors.converter]]
  namepass = ["spark_orchestration_logs"]
  [processors.converter.fields]
    tag = ["request_id", "iccid"]

## Extract specific patterns from messages
[[processors.regex]]
  namepass = ["spark_orchestration_logs"]

  ## Extract API URL
  [[processors.regex.tags]]
    key = "message"
    pattern = 'Initiate API Calling Process:\[(http[^\]]+)\]'
    replacement = "${1}"
    result_key = "api_url"

  ## Extract Response Code
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Response Code : (\d+)'
    replacement = "${1}"
    result_key = "response_code"

  ## Extract Record Count
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Record Count : (\d+)'
    replacement = "${1}"
    result_key = "record_count"

  ## Extract API Sequence
  [[processors.regex.fields]]
    key = "message"
    pattern = 'api_sequence: (\d+)'
    replacement = "${1}"
    result_key = "api_sequence"

## Extract database connection pool stats
[[processors.regex]]
  namepass = ["spark_orchestration_logs"]

  ## Extract HikariCP Total
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Total: (\d+)'
    replacement = "${1}"
    result_key = "hikari_total"

  ## Extract HikariCP Active
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Active: (\d+)'
    replacement = "${1}"
    result_key = "hikari_active"

  ## Extract HikariCP Idle
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Idle: (\d+)'
    replacement = "${1}"
    result_key = "hikari_idle"

  ## Extract HikariCP Waiting
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Waiting Threads: (\d+)'
    replacement = "${1}"
    result_key = "hikari_waiting"

## Extract identifiers (IMSI, MSISDN, EID)
[[processors.regex]]
  namepass = ["spark_orchestration_logs"]

  ## Extract IMSI
  [[processors.regex.tags]]
    key = "message"
    pattern = '"imsi":"(\d+)"'
    replacement = "${1}"
    result_key = "imsi"

  ## Extract MSISDN
  [[processors.regex.tags]]
    key = "message"
    pattern = '"msisdn":"(\d+)"'
    replacement = "${1}"
    result_key = "msisdn"

  ## Extract EID
  [[processors.regex.tags]]
    key = "message"
    pattern = '"eid":"(\d+)"'
    replacement = "${1}"
    result_key = "eid"

  ## Extract Enterprise ID
  [[processors.regex.tags]]
    key = "message"
    pattern = '"enterpriseId":"(\w+)"'
    replacement = "${1}"
    result_key = "enterprise_id"

  ## Extract Country
  [[processors.regex.tags]]
    key = "message"
    pattern = '"country":"(\w+)"'
    replacement = "${1}"
    result_key = "country"

## Detect orchestration phases
[[processors.regex]]
  namepass = ["spark_orchestration_logs"]

  ## Detect Start
  [[processors.regex.tags]]
    key = "message"
    pattern = 'Start Orchastration Processing'
    replacement = "start"
    result_key = "phase"

  ## Detect End
  [[processors.regex.tags]]
    key = "message"
    pattern = 'End Orchastration Processing'
    replacement = "end"
    result_key = "phase"

  ## Detect Connection Events
  [[processors.regex.tags]]
    key = "message"
    pattern = '(Before getConnection|After getConnection|Connection acquired|Connection closed)'
    replacement = "${1}"
    result_key = "db_event"

## Detect status and errors
[[processors.regex]]
  namepass = ["spark_orchestration_logs"]

  ## Detect HTTP methods
  [[processors.regex.tags]]
    key = "message"
    pattern = "Sending '(POST|GET|PUT|DELETE)' request"
    replacement = "${1}"
    result_key = "http_method"

  ## Detect Truncated operations
  [[processors.regex.tags]]
    key = "message"
    pattern = '(Truncated|Deleted \d+ rows)'
    replacement = "cleanup"
    result_key = "operation_type"

############## OUTPUTS Pri ##############
[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["port_listen","link_count","app_metrics","portResponsetime","ping","spark_*"]

[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "1hrRP"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["spark_orchestration_logs"]

{% if influx_Sec_IP is defined and influx_Sec_IP|length > 0 %}
############# OUTPUTS Sec ##################
[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["port_listen","link_count","app_metrics","portResponsetime","ping","spark_*"]

[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "1hrRP"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["spark_orchestration_logs"]

{% endif %}