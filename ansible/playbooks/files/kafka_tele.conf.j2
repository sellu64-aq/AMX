[global_tags]
  project = "{{ project }}"
  zone = "{{ zone }}"
  nodeIP = "{{ ansible_host }}"
  service = "kafka"

[agent]
  debug = true
  quiet = false
  interval = "30s"
  flush_interval = "10s"
  logfile = "{{ telegraf_remote_path }}/telegraf/logs/{{ telegraf_name }}_telegraf.log"
  hostname = "{{ inventory_hostname }}"
  omit_hostname = false
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "5s"
  flush_jitter = "5s"
  precision = "1ms"
  logfile_rotation_interval = "12h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 5

####### Port Status for {{ inventory_hostname }} #######
{% for item in ports %}
[[inputs.net_response]]
  protocol = "tcp"
  address = "{{ item.ip }}:{{ item.port }}"
  name_override = "portResponsetime"
  tags = { linkName="{{ item.name }}", ip="{{ item.ip }}", port="{{ item.port }}", host="{{ inventory_hostname }}" }
{% endfor %}

####### Port Status for {{ inventory_hostname }} #######
{% for item in ports %}
[[inputs.net_response]]
  protocol = "tcp"
  address = "{{ item.ip }}:{{ item.port }}"
  name_override = "portResponsetime"
  tags = { linkName="{{ item.name }}", ip="{{ item.ip }}", port="{{ item.port }}", host="{{ inventory_hostname }}" }
{% endfor %}

############## Ping Monitoring ################
[[inputs.ping]]
  {% set unique_ips = ports | map(attribute='ip') | unique %}
  urls = [{% for ip in unique_ips %}"{{ ip }}"{% if not loop.last %}, {% endif %}{% endfor %}]
  count = 2
  ping_interval = 3.0
  timeout = 2.0
  method = "exec"

##### App Metrics ###########
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_app_metrics.sh"]
  data_format = "influx"
  name_override = "app_metrics"
  interval = "1m"
  timeout = "50s"

############# Port Monitor #############
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_port_listen_monitor.sh"]
  timeout = "10s"
  data_format = "influx"
  name_override = "port_listen"

############# Port Monitor #############
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_port_conn_monitor.sh"]
  timeout = "10s"
  data_format = "influx"
  name_override = "link_count"

##################################

[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_consumergroup_current_offset"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_consumergroup_current_offset"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_consumergroup_current_offset"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_consumergroup_current_offset"]
  [[processors.rename.replace]]
    measurement = "kafka_consumergroup_current_offset"
    dest = "kafka_consumergroup_current_offset"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_broker_info"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_broker_info"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_broker_info"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_broker_info"]
  [[processors.rename.replace]]
    measurement = "kafka_broker_info"
    dest = "kafka_broker_info"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_brokers"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_brokers"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_brokers"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_brokers"]
  [[processors.rename.replace]]
    measurement = "kafka_brokers"
    dest = "kafka_brokers"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_consumergroup_current_offset_sum"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_consumergroup_current_offset_sum"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_consumergroup_current_offset_sum"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_consumergroup_current_offset_sum"]
  [[processors.rename.replace]]
    measurement = "kafka_consumergroup_current_offset_sum"
    dest = "kafka_consumergroup_current_offset_sum"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_consumergroup_lag"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_consumergroup_lag"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_consumergroup_lag"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_consumergroup_lag"]
  [[processors.rename.replace]]
    measurement = "kafka_consumergroup_lag"
    dest = "kafka_consumergroup_lag"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_consumergroup_lag_sum"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_consumergroup_lag_sum"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_consumergroup_lag_sum"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_consumergroup_lag_sum"]
  [[processors.rename.replace]]
    measurement = "kafka_consumergroup_lag_sum"
    dest = "kafka_consumergroup_lag_sum"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_consumergroup_members"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_consumergroup_members"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_consumergroup_members"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_consumergroup_members"]
  [[processors.rename.replace]]
    measurement = "kafka_consumergroup_members"
    dest = "kafka_consumergroup_members"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partition_current_offset"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partition_current_offset"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partition_current_offset"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partition_current_offset"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partition_current_offset"
    dest = "kafka_topic_partition_current_offset"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partition_in_sync_replica"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partition_in_sync_replica"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partition_in_sync_replica"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partition_in_sync_replica"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partition_in_sync_replica"
    dest = "kafka_topic_partition_in_sync_replica"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partition_leader"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partition_leader"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partition_leader"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partition_leader"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partition_leader"
    dest = "kafka_topic_partition_leader"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partition_leader_is_preferred"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partition_leader_is_preferred"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partition_leader_is_preferred"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partition_leader_is_preferred"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partition_leader_is_preferred"
    dest = "kafka_topic_partition_leader_is_preferred"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partition_oldest_offset"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partition_oldest_offset"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partition_oldest_offset"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partition_oldest_offset"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partition_oldest_offset"
    dest = "kafka_topic_partition_oldest_offset"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partition_replicas"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partition_replicas"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partition_replicas"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partition_replicas"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partition_replicas"
    dest = "kafka_topic_partition_replicas"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partitions"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partitions"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partitions"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partitions"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partitions"
    dest = "kafka_topic_partitions"
####################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9308/metrics"]
  name_override = "kafka_topic_partition_under_replicated_partition"
  metric_version = 2
  interval = "60s"

[[processors.starlark]]
  namepass = ["kafka_topic_partition_under_replicated_partition"]
  order = 0
  source = '''
def apply(metric):
    prefix = "kafka_topic_partition_under_replicated_partition"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)
    if not metric.fields:
        return None
    return metric
'''

[[processors.rename]]
  namepass = ["kafka_topic_partition_under_replicated_partition"]
  [[processors.rename.replace]]
    measurement = "kafka_topic_partition_under_replicated_partition"
    dest = "kafka_topic_partition_under_replicated_partition"
####################################################

########## OUTPUTS Pri ##############
[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["port_listen","link_count","app_metrics","portResponsetime","ping",
    "kafka_broker_info",
    "kafka_brokers",
    "kafka_consumergroup_current_offset",
    "kafka_consumergroup_current_offset_sum",
    "kafka_consumergroup_lag",
    "kafka_consumergroup_lag_sum",
    "kafka_topic_partition_current_offset",
    "kafka_topic_partition_in_sync_replica",
    "kafka_topic_partition_leader",
    "kafka_topic_partition_leader_is_preferred",
    "kafka_topic_partition_oldest_offset",
    "kafka_topic_partition_replicas",
    "kafka_topic_partitions",
    "kafka_topic_partition_under_replicated_partition"
  ]

[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "1hrRP"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["kafka_consumergroup_members"]

### OUTPUTS Sec #####

[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["port_listen","link_count","app_metrics","portResponsetime","ping",
    "kafka_broker_info",
    "kafka_brokers",
    "kafka_consumergroup_current_offset",
    "kafka_consumergroup_current_offset_sum",
    "kafka_consumergroup_lag",
    "kafka_consumergroup_lag_sum",
    "kafka_topic_partition_current_offset",
    "kafka_topic_partition_in_sync_replica",
    "kafka_topic_partition_leader",
    "kafka_topic_partition_leader_is_preferred",
    "kafka_topic_partition_oldest_offset",
    "kafka_topic_partition_replicas",
    "kafka_topic_partitions",
    "kafka_topic_partition_under_replicated_partition"
  ]

[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "1hrRP"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["kafka_consumergroup_members"]
