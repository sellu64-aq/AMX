[global_tags]
  project = "{{ project }}"
  zone = "{{ zone }}"
  nodeIP = "{{ ansible_host }}"
  service = "spark"
  stream = "notifyretry"

[agent]
  debug = true
  quiet = false
  interval = "30s"
  flush_interval = "30s"
  logfile = "{{ telegraf_remote_path }}/telegraf/logs/{{ telegraf_name }}_telegraf.log"
  hostname = "{{ inventory_hostname }}"
  omit_hostname = false
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 20000
  collection_jitter = "0s"
  flush_jitter = "0s"
  precision = "0s"
  logfile_rotation_interval = "12h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 5

#############################################################################
[[inputs.prometheus]]
 urls = ["http://{{ ansible_host }}:{{ spark_port }}/metrics"]
 name_override = "spark_raw"
 metric_version = 2
 interval = "50s"
 timeout = "30s"

###############################################################################
# PROCESSOR-1: CATEGORIZE AND NORMALIZE SPARK METRICS (GENERIC)
###############################################################################
[[processors.starlark]]
  namepass = ["spark_raw"]
  source = '''
def apply(metric):
    new_fields = {}
    new_tags = {}

    for key, value in metric.fields.items():
        # ================================================================
        # JVM METRICS
        # ================================================================
        if key.startswith("jvm_") or key.startswith("java_lang_"):
            metric.name = "spark_jvm"
            new_fields[key] = value

        # ================================================================
        # KAFKA CONSUMER METRICS
        # ================================================================
        elif key.startswith("kafka_consumer_"):
            metric.name = "spark_kafka"
            new_fields[key] = value

        # ================================================================
        # SPARK STREAMING METRICS - GENERIC EXTRACTION
        # ================================================================
        elif key.startswith("metrics_local_"):
            parts = key.split("_")

            # Strip metrics_local_<id>_driver_ prefix if present
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                stripped = "_".join(parts[4:])
            else:
                stripped = key

            # Check if this is a streaming metric
            if "streamingmetrics_streaming_" in stripped:
                metric.name = "spark_streaming"

                # Extract service name (everything before "streamingmetrics_streaming_")
                service_name = extract_service_name_generic(stripped)
                if service_name:
                    new_tags["streaming_service"] = service_name

                # Extract standard metric name (everything after "streamingmetrics_streaming_")
                standard_metric = extract_standard_metric(stripped)
                if standard_metric:
                    new_fields[standard_metric] = value

            # LiveListenerBus metrics
            elif "livelistenerbus" in stripped:
                metric.name = "spark_livelistenerbus"
                new_fields[stripped] = value

            # Code Generator metrics
            elif "codegenerator" in stripped:
                metric.name = "spark_codegenerator"
                new_fields[stripped] = value

            # Executor metrics
            elif "executor" in stripped:
                metric.name = "spark_executor"
                new_fields[stripped] = value

            # Block Manager metrics
            elif "blockmanager" in stripped:
                metric.name = "spark_blockmanager"
                new_fields[stripped] = value

            # JVM Pool metrics
            elif "jvm_pools" in stripped:
                metric.name = "spark_jvm_pools"
                new_fields[stripped] = value

            # Hive Catalog metrics
            elif "hiveexternalcatalog" in stripped:
                metric.name = "spark_hive"
                new_fields[stripped] = value

            # DAG Scheduler metrics
            elif "dagscheduler" in stripped:
                metric.name = "spark_dagscheduler"
                new_fields[stripped] = value

            # Other Spark metrics
            else:
                metric.name = "spark_other"
                new_fields[stripped] = value

        # ================================================================
        # MISCELLANEOUS METRICS
        # ================================================================
        else:
            metric.name = "spark_misc"
            new_fields[key] = value

    # Clear and update fields
    metric.fields.clear()
    for k, v in new_fields.items():
        metric.fields[k] = v

    # Add new tags
    for k, v in new_tags.items():
        metric.tags[k] = v

    return metric

# ================================================================
# HELPER FUNCTION: Extract service name generically
# ================================================================
def extract_service_name_generic(field_name):
    """
    Extract service name by taking everything BEFORE "streamingmetrics_streaming_"

    Examples:
      gmsa_orchastration_engine_notify_limit_streamingmetrics_streaming_...
        → gmsa_orchastration_engine_notify_limit

      orchestrationstreamingapi_gmsa_addorder_streamingmetrics_streaming_...
        → orchestrationstreamingapi_gmsa_addorder

      projectx_some_service_name_streamingmetrics_streaming_...
        → projectx_some_service_name
    """
    if "streamingmetrics_streaming_" in field_name:
        # Split on the marker and take everything before it
        service_name = field_name.split("streamingmetrics_streaming_")[0]
        # Remove trailing underscore if present
        service_name = service_name.rstrip("_")
        return service_name

    return "unknown"

# ================================================================
# HELPER FUNCTION: Extract standard metric name
# ================================================================
def extract_standard_metric(field_name):
    """
    Extract the standard metric name by taking everything AFTER "streamingmetrics_streaming_"
    This is the common suffix that all streaming services share.

    Examples:
      gmsa_orchastration_engine_notify_limit_streamingmetrics_streaming_lastcompletedbatch_processingdelay_value
        → streaming_lastcompletedbatch_processingdelay_value

      orchestrationstreamingapi_gmsa_addorder_streamingmetrics_streaming_waitingbatches_value
        → streaming_waitingbatches_value
    """
    if "streamingmetrics_streaming_" in field_name:
        # Split on the marker and take everything after it
        # Keep "streaming_" prefix for clarity
        standard_part = field_name.split("streamingmetrics_streaming_")[1]
        return "streaming_" + standard_part

    return field_name
'''

###############################################################################
# PROCESSOR-2: REMOVE NaN AND INFINITE VALUES
###############################################################################
[[processors.starlark]]
  namepass = ["spark_*"]
  source = '''
def apply(metric):
    remove = []
    for k, v in metric.fields.items():
        # Remove NaN and Inf values
        if str(v) == "nan" or v == float("inf") or v == float("-inf"):
            remove.append(k)

    # Remove invalid fields
    for k in remove:
        metric.fields.pop(k)

    return metric
'''

###############################################################################
# PROCESSOR-3: ADD COMPUTED FIELDS (OPTIONAL)
###############################################################################
[[processors.starlark]]
  namepass = ["spark_streaming"]
  source = '''
def apply(metric):
    """
    Add computed fields for better monitoring:
    - batch_efficiency: processingdelay / totaldelay
    - is_lagging: 1 if waitingbatches > 0 else 0
    - processing_rate: records per millisecond
    """
    fields = metric.fields

    # Calculate batch processing efficiency
    processing_delay = fields.get("streaming_lastcompletedbatch_processingdelay_value", 0)
    total_delay = fields.get("streaming_lastcompletedbatch_totaldelay_value", 1)

    if total_delay > 0:
        fields["batch_efficiency_percent"] = (processing_delay / total_delay) * 100

    # Add lag indicator
    waiting_batches = fields.get("streaming_waitingbatches_value", 0)
    fields["is_lagging"] = 1 if waiting_batches > 0 else 0

    # Add batch throughput (records / processing time)
    total_records = fields.get("streaming_totalprocessedrecords_value", 0)
    total_batches = fields.get("streaming_totalcompletedbatches_value", 1)

    if total_batches > 0:
        fields["avg_records_per_batch"] = total_records / total_batches

    # Calculate processing rate (if we have timing data)
    last_batch_records = fields.get("streaming_lastreceivedbatch_records_value", 0)
    if processing_delay > 0 and last_batch_records > 0:
        fields["records_per_second"] = (last_batch_records / processing_delay) * 1000

    return metric
'''

######### Notification Logs ###########Sample logs below #####
#2025-10-22 08:41:23 INFO  INFO:37 :  - Configuration:- Response_Topic_nb solicited_notification_gmsa
#2025-10-22 08:41:23 INFO  INFO:38 :  - Configuration:- cmp-prod-kafka-gmsa-01:2181,cmp-prod-kafka-gmsa-02:2181,cmp-prod-kafka-gmsa-03:2181,cmp-prod-kafka-gmsa-04:2181,cmp-prod-kafka-gmsa-05:2181 test 1
#2025-10-22 08:41:23 INFO  INFO:40 :  - Configuration:- com.mysql.jdbc.Driver jdbc:mysql://cmp-mysql-router:6446/gmsa_orchestration_claro_nb?user=aqadmin&password=Aqadmin@123
#2025-10-22 08:41:29 WARN  NativeCodeLoader:62 :  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
#2025-10-22 08:41:36 INFO  log:192 :  - Logging initialized @20444ms
#########################################################
[[inputs.tail]]  ### -->  ######### notification Logs  ##########
  ## Spark Notification log file
  files = ["{{ stream_log_path }}/logging.log"]

  ## Read from beginning or current position
  initial_read_offset = "beginning"

  ## File watch method
  watch_method = "inotify"

  ## Data parsing format
  data_format = "grok"

  ## Measurement name
  name_override = "spark_notification_logs"

  ## Grok patterns for Spark logs
  grok_patterns = [
    # Standard Spark log format: Date Time LEVEL Class:Line - Message
    "^%{TIMESTAMP_ISO8601:log_time} %{LOGLEVEL:log_level}\\s+%{JAVACLASS:class}:%{NUMBER:line_number} :\\s+- %{GREEDYDATA:message}$",

    # Spark log without line number
    "^%{TIMESTAMP_ISO8601:log_time} %{LOGLEVEL:log_level}\\s+%{JAVACLASS:class} :\\s+- %{GREEDYDATA:message}$",

    # Kafka/Spark component logs
    "^%{TIMESTAMP_ISO8601:log_time} %{LOGLEVEL:log_level}\\s+%{DATA:component}:%{NUMBER:line_number} :\\s+- %{GREEDYDATA:message}$",

    # Generic fallback
    "^%{TIMESTAMP_ISO8601:log_time} %{LOGLEVEL:log_level}\\s+%{GREEDYDATA:message}$"
  ]

  ## Custom grok patterns
  grok_custom_patterns = '''
    LOGLEVEL (TRACE|DEBUG|INFO|WARN|ERROR|FATAL)
    JAVACLASS ([a-zA-Z0-9$._]+)
  '''

  ## Timezone (if needed)
  grok_timezone = "UTC"

  ## Additional tags
  [inputs.tail.tags]
    application = "spark_streaming"
    service = "notification_service"
    component = "spark"

## Convert fields to tags for better indexing
[[processors.converter]]
  namepass = ["spark_notification_logs"]
  [processors.converter.fields]
    tag = ["log_level", "class", "component"]

## Extract specific patterns from messages
[[processors.regex]]
  namepass = ["spark_notification_logs"]

  ## Extract RequestId
  [[processors.regex.tags]]
    key = "message"
    pattern = 'RequestId:\[([^\]]+)\]'
    replacement = "${1}"
    result_key = "request_id"

  ## Extract IMSI
  [[processors.regex.tags]]
    key = "message"
    pattern = 'Imsi is:\[([^\]]+)\]'
    replacement = "${1}"
    result_key = "imsi"

  ## Extract ICCID
  [[processors.regex.tags]]
    key = "message"
    pattern = 'ICCID is:\[([^\]]+)\]'
    replacement = "${1}"
    result_key = "iccid"

  ## Extract Event Type
  [[processors.regex.tags]]
    key = "message"
    pattern = '/events/(\w+)'
    replacement = "${1}"
    result_key = "event_type"

  ## Extract Response Code (as field, not tag - will be string)
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Response Code:\[(\d+)\]'
    replacement = "${1}"
    result_key = "response_code"

## Extract Kafka-specific metrics
[[processors.regex]]
  namepass = ["spark_notification_logs"]

  ## Extract Record Count (as field - will be string)
  [[processors.regex.fields]]
    key = "message"
    pattern = 'Record Count : (\d+)'
    replacement = "${1}"
    result_key = "record_count"

  ## Extract Kafka version
  [[processors.regex.tags]]
    key = "message"
    pattern = 'Kafka version : ([0-9.]+)'
    replacement = "${1}"
    result_key = "kafka_version"

## Detect status from message content
[[processors.regex]]
  namepass = ["spark_notification_logs"]

  ## Detect failure
  [[processors.regex.tags]]
    key = "message"
    pattern = '(Failiure|Error Msg|connect timed out|failed)'
    replacement = "failure"
    result_key = "status"

  ## Detect streaming phase - start
  [[processors.regex.tags]]
    key = "message"
    pattern = 'Streaming Start'
    replacement = "start"
    result_key = "phase"

  ## Detect streaming phase - end
  [[processors.regex.tags]]
    key = "message"
    pattern = 'Streaming END'
    replacement = "end"
    result_key = "phase"

  ## Detect retry
  [[processors.regex.tags]]
    key = "message"
    pattern = '(Sent for Retry|OLNotificationRetry_nb)'
    replacement = "true"
    result_key = "retry"

############## OUTPUTS Pri ##############
[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["port_listen","link_count","app_metrics","portResponsetime","ping","spark_*"]

[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "1hrRP"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["spark_notification_logs"]

{% if influx_Sec_IP is defined and influx_Sec_IP|length > 0 %}
############# OUTPUTS Sec ##################
[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["port_listen","link_count","app_metrics","portResponsetime","ping","spark_*"]

[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "1hrRP"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["spark_notification_logs"]

{% endif %}