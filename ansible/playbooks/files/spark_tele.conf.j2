[global_tags]
  project = "{{ project }}"
  zone = "{{ zone }}"
  nodeIP = "{{ ansible_host }}"
  service = "spark"

[agent]
  debug = true
  quiet = false
  interval = "30s"
  flush_interval = "30s"
  logfile = "{{ telegraf_remote_path }}/telegraf/logs/{{ telegraf_name }}_telegraf.log"
  hostname = "{{ inventory_hostname }}"
  omit_hostname = false
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 20000
  collection_jitter = "0s"
  flush_jitter = "0s"
  precision = "0s"
  logfile_rotation_interval = "12h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 5

#############################################################################
[[inputs.prometheus]]
  urls = ["http://{{ ansible_host }}:9091/metrics"]
  name_override = "Spark_CodeGenerator"
  metric_version = 2
  interval = "30s"

# Step 1: Keep only CodeGenerator-related metrics
[[processors.starlark]]
  namepass = ["Spark_CodeGenerator"]
  order = 0
  source = '''
def apply(metric):
    # Only keep fields that match expected pattern
    keys_to_remove = []
    for key in metric.fields:
        if "_driver_codegenerator_" not in key:
            keys_to_remove.append(key)

    for k in keys_to_remove:
        metric.fields.pop(k)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_CodeGenerator"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_CodeGenerator"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_CodeGenerator"]
  [[processors.rename.replace]]
    measurement = "Spark_CodeGenerator"
    dest = "Spark_CodeGenerator"
#############################################################################
[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_BlockManager"
  metric_version = 2
  interval = "30s"

# Step 1: Keep only CodeGenerator-related metrics
[[processors.starlark]]
  namepass = ["Spark_BlockManager"]
  order = 0
  source = '''
def apply(metric):
    # Only keep fields that match expected pattern
    keys_to_remove = []
    for key in metric.fields:
        if "_driver_blockmanager_" not in key:
            keys_to_remove.append(key)

    for k in keys_to_remove:
        metric.fields.pop(k)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_BlockManager"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_BlockManager"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_BlockManager"]
  [[processors.rename.replace]]
    measurement = "Spark_BlockManager"
    dest = "Spark_BlockManager"

#############################################################################
[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_DAGscheduler"
  metric_version = 2
  interval = "30s"

# Step 1: Keep only CodeGenerator-related metrics
[[processors.starlark]]
  namepass = ["Spark_DAGscheduler"]
  order = 0
  source = '''
def apply(metric):
    # Only keep fields that match expected pattern
    keys_to_remove = []
    for key in metric.fields:
        if "_driver_dagscheduler_" not in key:
            keys_to_remove.append(key)

    for k in keys_to_remove:
        metric.fields.pop(k)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_DAGscheduler"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_DAGscheduler"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_DAGscheduler"]
  [[processors.rename.replace]]
    measurement = "Spark_DAGscheduler"
    dest = "Spark_DAGscheduler"

#############################################################################
[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_Hiveexternalcatalog"
  metric_version = 2
  interval = "30s"

# Step 1: Keep only CodeGenerator-related metrics
[[processors.starlark]]
  namepass = ["Spark_Hiveexternalcatalog"]
  order = 0
  source = '''
def apply(metric):
    # Only keep fields that match expected pattern
    keys_to_remove = []
    for key in metric.fields:
        if "_driver_hiveexternalcatalog_" not in key:
            keys_to_remove.append(key)

    for k in keys_to_remove:
        metric.fields.pop(k)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_Hiveexternalcatalog"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_Hiveexternalcatalog"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_Hiveexternalcatalog"]
  [[processors.rename.replace]]
    measurement = "Spark_Hiveexternalcatalog"
    dest = "Spark_Hiveexternalcatalog"


#############################################################################
[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_JVM"
  metric_version = 2
  interval = "30s"

# Step 1: Keep only CodeGenerator-related metrics
[[processors.starlark]]
  namepass = ["Spark_JVM"]
  order = 0
  source = '''
def apply(metric):
    # Only keep fields that match expected pattern
    keys_to_remove = []
    for key in metric.fields:
        if "_driver_jvm_" not in key:
            keys_to_remove.append(key)

    for k in keys_to_remove:
        metric.fields.pop(k)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_JVM"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_JVM"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_JVM"]
  [[processors.rename.replace]]
    measurement = "Spark_JVM"
    dest = "Spark_JVM"

#############################################################################
[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_Livelistenerbus"
  metric_version = 2
  interval = "30s"

# Step 1: Keep only CodeGenerator-related metrics
[[processors.starlark]]
  namepass = ["Spark_Livelistenerbus"]
  order = 0
  source = '''
def apply(metric):
    # Only keep fields that match expected pattern
    keys_to_remove = []
    for key in metric.fields:
        if "_driver_livelistenerbus_" not in key:
            keys_to_remove.append(key)

    for k in keys_to_remove:
        metric.fields.pop(k)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_Livelistenerbus"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_Livelistenerbus"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_Livelistenerbus"]
  [[processors.rename.replace]]
    measurement = "Spark_Livelistenerbus"
    dest = "Spark_Livelistenerbus"

#############################################################################
[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_OrchestrationStreamingAPI"
  metric_version = 2
  interval = "30s"

# Step 1: Keep only CodeGenerator-related metrics
[[processors.starlark]]
  namepass = ["Spark_OrchestrationStreamingAPI"]
  order = 0
  source = '''
def apply(metric):
    # Only keep fields that match expected pattern
    keys_to_remove = []
    for key in metric.fields:
        if "_driver_orchestrationstreamingapi" not in key:
            keys_to_remove.append(key)

    for k in keys_to_remove:
        metric.fields.pop(k)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_OrchestrationStreamingAPI"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_OrchestrationStreamingAPI"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_OrchestrationStreamingAPI"]
  [[processors.rename.replace]]
    measurement = "Spark_OrchestrationStreamingAPI"
    dest = "Spark_OrchestrationStreamingAPI"

#############################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_kafka_consumer_metrics"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_kafka_consumer_metrics"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "kafka_consumer_consumer_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_kafka_consumer_metrics"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_kafka_consumer_metrics"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_kafka_consumer_metrics"]
  [[processors.rename.replace]]
    measurement = "Spark_kafka_consumer_metrics"
    dest = "Spark_kafka_consumer_metrics"

#############################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_memorypool"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_memorypool"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_memorypool_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_memorypool"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_memorypool"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_memorypool"]
  [[processors.rename.replace]]
    measurement = "Spark_java_metrics"
    dest = "Spark_java_metrics"

##################################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_classloading"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_classloading"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_classloading_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_classloading"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_classloading"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_classloading"]
  [[processors.rename.replace]]
    measurement = "Spark_java_classloading"
    dest = "Spark_java_classloading"

##################################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_compilation"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_compilation"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_compilation_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_compilation"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_compilation"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_compilation"]
  [[processors.rename.replace]]
    measurement = "Spark_java_compilation"
    dest = "Spark_java_compilation"

##################################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_garbagecollector"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_garbagecollector"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_garbagecollector_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_garbagecollector"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_garbagecollector"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_garbagecollector"]
  [[processors.rename.replace]]
    measurement = "Spark_java_garbagecollector"
    dest = "Spark_java_garbagecollector"

#################################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_memory"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_memory"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_memory_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_memory"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_memory"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_memory"]
  [[processors.rename.replace]]
    measurement = "Spark_java_memory"
    dest = "Spark_java_memory"


#################################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_operatingSystem"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_operatingSystem"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_operatingsystem_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_operatingSystem"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_operatingSystem"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_operatingSystem"]
  [[processors.rename.replace]]
    measurement = "Spark_java_operatingSystem"
    dest = "Spark_java_operatingSystem"

#################################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_runtime"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_runtime"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_runtime_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_runtime"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_runtime"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_runtime"]
  [[processors.rename.replace]]
    measurement = "Spark_java_runtime"
    dest = "Spark_java_runtime"

#################################################################################

[[inputs.prometheus]]
  urls = ["http://10.221.81.43:9091/metrics"]
  name_override = "Spark_java_threading"
  metric_version = 2
  interval = "30s"

[[processors.starlark]]
  namepass = ["Spark_java_threading"]
  order = 0
  source = '''
def apply(metric):
    # Keep only fields that start with kafka_consumer_consumer_
    prefix = "java_lang_threading_"
    keys_to_remove = []
    for key in metric.fields:
        if not key.startswith(prefix):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 2: Remove NaN/Inf values
[[processors.starlark]]
  namepass = ["Spark_java_threading"]
  order = 1
  source = '''
def apply(metric):
    keys_to_remove = []
    for key, val in metric.fields.items():
        if str(val) == "nan" or val == float("inf") or val == float("-inf"):
            keys_to_remove.append(key)
    for key in keys_to_remove:
        metric.fields.pop(key)

    if not metric.fields:
        return None

    return metric
'''

# Step 3: Strip "metrics_local_<id>_driver_" from field names
[[processors.starlark]]
  namepass = ["Spark_java_threading"]
  order = 2
  source = '''
def apply(metric):
    new_fields = {}
    for k, v in metric.fields.items():
        if k.startswith("metrics_local_"):
            parts = k.split("_")
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                new_key = "_".join(parts[4:])
                new_fields[new_key] = v
            else:
                new_fields[k] = v
        else:
            new_fields[k] = v

    metric.fields.clear()
    for new_key, v in new_fields.items():
        metric.fields[new_key] = v

    return metric
'''

# Step 4: Rename measurement to keep it consistent
[[processors.rename]]
  namepass = ["Spark_java_threading"]
  [[processors.rename.replace]]
    measurement = "Spark_java_threading"
    dest = "Spark_java_threading"



############## OUTPUTS Pri ##############
[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = [
    "Spark_BlockManager",
    "Spark_CodeGenerator",
    "Spark_DAGscheduler",
    "Spark_Hiveexternalcatalog",
    "Spark_JVM",
    "Spark_Livelistenerbus",
    "Spark_OrchestrationStreamingAPI",
    "Spark_java_classloading",
    "Spark_java_compilation",
    "Spark_java_garbagecollector",
    "Spark_java_memory",
    "Spark_java_memorypool",
    "Spark_java_operatingSystem",
    "Spark_java_runtime",
    "Spark_java_threading",
    "Spark_jmx_metrics",
    "Spark_kafka_consumer_metrics"
  ]


############# OUTPUTS Sec ##################
[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = [
    "Spark_BlockManager",
    "Spark_CodeGenerator",
    "Spark_DAGscheduler",
    "Spark_Hiveexternalcatalog",
    "Spark_JVM",
    "Spark_Livelistenerbus",
    "Spark_OrchestrationStreamingAPI",
    "Spark_java_classloading",
    "Spark_java_compilation",
    "Spark_java_garbagecollector",
    "Spark_java_memory",
    "Spark_java_memorypool",
    "Spark_java_operatingSystem",
    "Spark_java_runtime",
    "Spark_java_threading",
    "Spark_jmx_metrics",
    "Spark_kafka_consumer_metrics"
  ]


