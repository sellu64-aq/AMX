[global_tags]
  project = "{{ project }}"
  zone = "{{ zone }}"
  nodeIP = "{{ ansible_host }}"
  service = "spark"

[agent]
  debug = true
  quiet = false
  interval = "30s"
  flush_interval = "30s"
  logfile = "{{ telegraf_remote_path }}/telegraf/logs/{{ telegraf_name }}_telegraf.log"
  hostname = "{{ inventory_hostname }}"
  omit_hostname = false
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 20000
  collection_jitter = "0s"
  flush_jitter = "0s"
  precision = "0s"
  logfile_rotation_interval = "12h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 5

####### Port Status for {{ inventory_hostname }} #######
{% for item in ports %}
[[inputs.net_response]]
  protocol = "tcp"
  address = "{{ item.ip }}:{{ item.port }}"
  name_override = "portResponsetime"
  tags = { linkName="{{ item.name }}", ip="{{ item.ip }}", port="{{ item.port }}", host="{{ inventory_hostname }}" }
{% endfor %}

####### Port Status for {{ inventory_hostname }} #######
{% for item in ports %}
[[inputs.net_response]]
  protocol = "tcp"
  address = "{{ item.ip }}:{{ item.port }}"
  name_override = "portResponsetime"
  tags = { linkName="{{ item.name }}", ip="{{ item.ip }}", port="{{ item.port }}", host="{{ inventory_hostname }}" }
{% endfor %}

############## Ping Monitoring ################
[[inputs.ping]]
  {% set unique_ips = ports | map(attribute='ip') | unique %}
  urls = [{% for ip in unique_ips %}"{{ ip }}"{% if not loop.last %}, {% endif %}{% endfor %}]
  count = 2
  ping_interval = 3.0
  timeout = 2.0
  method = "exec"

##### App Metrics ###########
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_app_metrics.sh"]
  data_format = "influx"
  name_override = "app_metrics"
  interval = "1m"
  timeout = "50s"

############# Port Monitor #############
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_port_listen_monitor.sh"]
  timeout = "10s"
  data_format = "influx"
  name_override = "port_listen"

############# Port Monitor #############
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_port_conn_monitor.sh"]
  timeout = "10s"
  data_format = "influx"
  name_override = "link_count"

##################################

#############################################################################
[[inputs.prometheus]]
 urls = ["http://{{ ansible_host }}:9091/metrics"]
 name_override = "spark_raw"
 metric_version = 2
 interval = "30s"

###############################################################################
# PROCESSOR-1: REWRITE SPARK METRICS â†’ spark_<category>
###############################################################################
[[processors.starlark]]
  namepass = ["spark_raw"]
  source = '''
def apply(metric):
    new_fields = {}

    for key, value in metric.fields.items():

        # ---------------- JVM ----------------
        if key.startswith("jvm_") or key.startswith("java_lang_"):
            metric.name = "spark_jvm"
            new_fields[key] = value

        # ---------------- Kafka ---------------
        elif key.startswith("kafka_consumer_"):
            metric.name = "spark_kafka"
            new_fields[key] = value

        # ---------------- Spark JMX ------------
        elif key.startswith("metrics_local_"):

            parts = key.split("_")

            # strip metrics_local_<id>_driver_
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                stripped = "_".join(parts[4:])
            else:
                stripped = key

            # LiveListenerBus (labels or no labels)
            if "livelistenerbus" in stripped:
                metric.name = "spark_livelistenerbus"

            elif "codegenerator" in stripped:
                metric.name = "spark_codegenerator"

            elif "executor" in stripped:
                metric.name = "spark_executor"

            elif "blockmanager" in stripped:
                metric.name = "spark_blockmanager"

            elif "streamingmetrics" in stripped or "streaminglistenerbus" in stripped or "streaming" in stripped:
                metric.name = "spark_streaming"

            elif "jvm_pools" in stripped:
                metric.name = "spark_jvm_pools"

            elif "hiveexternalcatalog" in stripped:
                metric.name = "spark_hive"

            elif "dagscheduler" in stripped:
                metric.name = "spark_dagscheduler"

            else:
                metric.name = "spark_other"

            new_fields[stripped] = value

        # ---------------- Misc ------------------
        else:
            metric.name = "spark_misc"
            new_fields[key] = value

    metric.fields.clear()
    for k,v in new_fields.items():
        metric.fields[k] = v

    return metric
'''

###############################################################################
# PROCESSOR-2: REMOVE NaN/INF
###############################################################################
[[processors.starlark]]
  namepass = ["spark_*"]
  source = '''
def apply(metric):
    remove = []
    for k,v in metric.fields.items():
        if str(v) == "nan" or v == float("inf") or v == float("-inf"):
            remove.append(k)
    for k in remove:
        metric.fields.pop(k)
    return metric
'''

############## OUTPUTS Pri ##############
[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
  namepass = ["port_listen","link_count","app_metrics","portResponsetime","ping","spark_*"]

{% if influx_Sec_IP is defined and influx_Sec_IP|length > 0 %}
############# OUTPUTS Sec ##################
[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
{% endif %}