[global_tags]
  project = "{{ project }}"
  zone = "{{ zone }}"
  nodeIP = "{{ ansible_host }}"
  service = "spark"

[agent]
  debug = true
  quiet = false
  interval = "30s"
  flush_interval = "30s"
  logfile = "{{ telegraf_remote_path }}/telegraf/logs/{{ telegraf_name }}_telegraf.log"
  hostname = "{{ inventory_hostname }}"
  omit_hostname = false
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 20000
  collection_jitter = "0s"
  flush_jitter = "0s"
  precision = "0s"
  logfile_rotation_interval = "12h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 5

#############################################################################
[[inputs.prometheus]]
 urls = ["http://{{ ansible_host }}:9091/metrics"]
 name_override = "spark_raw"
 metric_version = 2
 interval = "30s"

###############################################################################
# PROCESSOR-1: REWRITE SPARK METRICS â†’ spark_<category>
###############################################################################
[[processors.starlark]]
  namepass = ["spark_raw"]
  source = '''
def apply(metric):
    new_fields = {}

    for key, value in metric.fields.items():

        # ---------------- JVM ----------------
        if key.startswith("jvm_") or key.startswith("java_lang_"):
            metric.name = "spark_jvm"
            new_fields[key] = value

        # ---------------- Kafka ---------------
        elif key.startswith("kafka_consumer_"):
            metric.name = "spark_kafka"
            new_fields[key] = value

        # ---------------- Spark JMX ------------
        elif key.startswith("metrics_local_"):

            parts = key.split("_")

            # strip metrics_local_<id>_driver_
            if len(parts) > 4 and parts[2].isdigit() and parts[3] == "driver":
                stripped = "_".join(parts[4:])
            else:
                stripped = key

            # LiveListenerBus (labels or no labels)
            if "livelistenerbus" in stripped:
                metric.name = "spark_livelistenerbus"

            elif "codegenerator" in stripped:
                metric.name = "spark_codegenerator"

            elif "executor" in stripped:
                metric.name = "spark_executor"

            elif "blockmanager" in stripped:
                metric.name = "spark_blockmanager"

            elif "streamingmetrics" in stripped or "streaminglistenerbus" in stripped or "streaming" in stripped:
                metric.name = "spark_streaming"

            elif "jvm_pools" in stripped:
                metric.name = "spark_jvm_pools"

            elif "hiveexternalcatalog" in stripped:
                metric.name = "spark_hive"

            elif "dagscheduler" in stripped:
                metric.name = "spark_dagscheduler"

            else:
                metric.name = "spark_other"

            new_fields[stripped] = value

        # ---------------- Misc ------------------
        else:
            metric.name = "spark_misc"
            new_fields[key] = value

    metric.fields.clear()
    for k,v in new_fields.items():
        metric.fields[k] = v

    return metric
'''

###############################################################################
# PROCESSOR-2: REMOVE NaN/INF
###############################################################################
[[processors.starlark]]
  namepass = ["spark_*"]
  source = '''
def apply(metric):
    remove = []
    for k,v in metric.fields.items():
        if str(v) == "nan" or v == float("inf") or v == float("-inf"):
            remove.append(k)
    for k in remove:
        metric.fields.pop(k)
    return metric
'''

############## OUTPUTS Pri ##############
[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true

{% if influx_Sec_IP is defined and influx_Sec_IP|length > 0 %}
############# OUTPUTS Sec ##################
[[outputs.influxdb]]
  urls = ["http://{{ influx_Sec_IP }}:{{ influx_Sec_Port }}"]
  database = "{{ influx_secDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  skip_database_creation = true
{% endif %}