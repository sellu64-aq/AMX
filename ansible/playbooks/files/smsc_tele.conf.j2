[global_tags]
  service = "smsc"
  project = "{{ project }}"
  zone = "{{ zone }}"
  nodeIP = "{{ ansible_host }}"

[agent]
  debug = true
  quiet = false
  interval = "30s"
  flush_interval = "30s"
  logfile = "{{ telegraf_remote_path }}/telegraf/logs/{{ telegraf_name }}_telegraf.log"
  hostname = "{{ inventory_hostname }}"
  omit_hostname = false
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 20000
  collection_jitter = "0s"
  flush_jitter = "0s"
  precision = "0s"
  logfile_rotation_interval = "12h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 5

####### Port Status for {{ inventory_hostname }} #######
{% for item in ports %}
[[inputs.net_response]]
  protocol = "tcp"
  address = "{{ item.ip }}:{{ item.port }}"
  name_override = "portResponsetime"
  tags = { linkName="{{ item.name }}", ip="{{ item.ip }}", port="{{ item.port }}", host="{{ inventory_hostname }}" }
{% endfor %}

############# Link Count #############
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_port_conn_monitor.sh"]
  timeout = "10s"
  data_format = "influx"
  name_override = "link_count"

############# Port Monitor #############
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_port_listen_monitor.sh"]
  timeout = "10s"
  data_format = "influx"
  name_override = "port_listen"

############## Ping Monitoring ################
[[inputs.ping]]
  {% set unique_ips = ports | map(attribute='ip') | unique %}
  urls = [{% for ip in unique_ips %}"{{ ip }}"{% if not loop.last %}, {% endif %}{% endfor %}]
  count = 2
  ping_interval = 3.0
  timeout = 2.0
  method = "exec"

##### App Metrics ###########
[[inputs.exec]]
  commands = ["{{ telegraf_remote_path }}/scripts/{{ telegraf_name }}_app_metrics.sh"]
  data_format = "influx"
  name_override = "app_metrics"
  interval = "1m"
  timeout = "50s"

########## Tails ################
[[inputs.tail]]
  files = ["/opt/Roamware/logs/cloudsimsmsc/traps/SMSC*-traps.txt"]
  name_override = "SNMP"
  data_format = "grok"
  watch_method = "inotify"
  initial_read_offset = "beginning"

  grok_patterns = [
    '''%{DATA:dra_timestamp},%{DATA:host},%{DATA:device},%{GREEDYDATA:kvdata}'''
  ]

[[processors.starlark]]
  namepass = ["SNMP"]
  source = '''
def apply(metric):
    raw_log = metric.fields["kvdata"]
    parts = raw_log.split(',')
    key_value_pairs = []

    for part in parts:
        if "=" in part:
            key, value = part.split("=", 1)
            key_value_pairs.append((key.strip(), value.strip()))

    oid_map = {
        "1.3.6.1.4.1.11150.1.3.10": "AlarmTime",
        "1.3.6.1.4.1.11150.1.3.1": "severity",
        "1.3.6.1.4.1.11150.1.3.2": "description",
        "1.3.6.1.4.1.11150.1.3.13": "ServiceName",
        "1.3.6.1.4.1.11150.1.3.14": "AlarmType",
        "1.3.6.1.4.1.11150.1.3.17": "ProbableCause",
        "1.3.6.1.4.1.11150.1.3.21": "NotificationId",
        "1.3.6.1.4.1.11150.1.3.23": "SequenceNumber",
        "1.3.6.1.4.1.11150.1.3.3": "MachineID",
        "1.3.6.1.4.1.11150.1.3.39": "ResourceName",
        "1.3.6.1.4.1.11150.1.3.41": "IP",
        "1.3.6.1.4.1.11150.1.3.8": "Port",
        "1.3.6.1.4.1.11150.84.1.1.6": "Application",
        "trapName": "trapName",
        "trap": "trap",
        "enterprise": "enterprise"
    }

    tag_fields = ["MachineID", "ServiceName", "description", "severity"]

    for key, value in key_value_pairs:
        field_name = oid_map.get(key, key)
        clean_value = value.replace(" ", "_").replace("\\t", "_")
        if field_name in tag_fields:
            metric.tags[field_name] = clean_value
        else:
            metric.fields[field_name] = clean_value

    return metric
  '''

########## Tails ################
[[inputs.tail]]
  files = ["/opt/Roamware/logs/cloudsimsmsc/logs/smsc.log"]
  name_override = "smsc_log"
  data_format = "grok"
  initial_read_offset = "end"
  watch_method = "inotify"
  grok_patterns = [
    '''%{DAY:day} %{MONTH:month} %{MONTHDAY:monthday}\|%{TIME:time}\|%{WORD:log_level}\|%{DATA:thread}\|%{DATA:message}\|%{JAVACLASS:class}\|%{WORD:method}\|%{NUMBER:line:int}'''
  ]
  grok_custom_patterns = '''
    JAVACLASS (?:[a-zA-Z_][\w$]*\.)+[A-Za-z_$][\w$]*
  '''

  # Specify which fields to treat as tags
  tagexclude = []
  tagpass = {}
  taginclude = ["log_level", "thread", "class", "method", "project", "zone", "service"]

[[processors.converter]]
  namepass = ["smsc_log"]
  [processors.converter.fields]
    tag = ["log_level", "class", "method"]

########## SmscOnlineCDR (cdr / key 16) ################
[[inputs.tail]]
  files = ["/opt/airlinq/grafana/scripts/LATEST_SmscOnlineCDR.xls"] #<--># "/opt/Roamware/logs/cloudsimsmsc/cdr/SmscOnlineCDR.*.xls"]
  name_override = "smsc_cdr"
  initial_read_offset = "end"
  watch_method = "inotify"

  data_format = "csv"
  csv_delimiter = ","
  csv_header_row_count = 0

  csv_column_names = [
    "record_time",
    "smsc_name",
    "key",
    "fsm_id",
    "record_type",
    "orig_address",
    "dest_address",
    "msg_rcv_time",
    "last_attempt_time",
    "dest_imsi",
    "smsc_address",
    "delivery_status",
    "attempts",
    "error_code",
    "reason_code",
    "network_id",
    "msc_address",
    "origination_gt",
    "notification_url",
    "quota_type",
    "quota_id",
    "called_no_type",
    "called_no_mcc",
    "extra_1","extra_2","extra_3","extra_4","extra_5",
    "extra_6","extra_7","extra_8","extra_9","extra_10"
  ]

  csv_timestamp_column = "record_time"
  csv_timestamp_format = "02-Jan-2006 15:04:05.000"

  csv_tag_columns = [
    "smsc_name",
    "record_type",
    "delivery_status"
  ]

########## SmscStatus (status / key 17) ################
[[inputs.tail]]
  files = ["/opt/airlinq/grafana/scripts/LATEST_StatusSMS.xls"] #<-># "/opt/Roamware/logs/cloudsimsmsc/SmscStatus/StatusSMS.*.xls"]
  name_override = "smsc_status"
  initial_read_offset = "end"
  watch_method = "inotify"

  data_format = "csv"
  csv_delimiter = ","
  csv_header_row_count = 0

  csv_column_names = [
    "record_time",
    "smsc_name",
    "key",
    "fsm_id",
    "record_type",
    "da_label",
    "dest_address",
    "imsi_label",
    "dest_imsi",
    "oa_label",
    "orig_address",
    "msc_label",
    "msc_address",
    "hlr_label",
    "hlr_address",
    "status_label",
    "delivery_status",
    "error_code_label",
    "error_code",
    "reason_code_label",
    "reason_code",
    "retry_conf",
    "attempts"
  ]

  csv_timestamp_column = "record_time"
  csv_timestamp_format = "02-Jan-2006 15:04:05.000"

  csv_tag_columns = [
    "smsc_name",
    "record_type",
    "delivery_status"
  ]

########## HttpSmsCDR ################
[[inputs.tail]]
  files = ["/opt/airlinq/grafana/scripts/LATEST_HttpSmsCDR.xls"]
  name_override = "smsc_http_cdr"

  ## Critical for symlinks - don't use initial_read_offset
  initial_read_offset = "beginning"

  ## Use poll method instead of inotify for symlinks
  watch_method = "poll"

  ## Reopen files that have been moved/renamed
  pipe = false

  data_format = "csv"
  csv_delimiter = ","
  csv_header_row_count = 0
  csv_skip_rows = 0
  csv_trim_space = true

  csv_column_names = [
    "record_time",
    "smsc_name",
    "key",
    "fsm_id",
    "http_msg_id",
    "result_code",
    "result_desc",
    "submit_time",
    "delivery_time",
    "dest_address",
    "network_id",
    "error_code",
    "extra_field"
  ]

  csv_timestamp_column = "record_time"
  csv_timestamp_format = "02-Jan-2006 15:04:05.000"

  csv_tag_columns = [
    "smsc_name",
    "result_code",
    "network_id"
  ]

############## OUTPUTS Pri ##############
[[outputs.influxdb]]
  urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
  database = "{{ influx_priDB_name }}"
  retention_policy = "thirty_days"
  timeout = "5s"
  namepass = ["app_metrics","IPPort_Status","portResponsetime","ip_listener_status","SNMP"]
  skip_database_creation = true

 [[outputs.influxdb]]
   urls = ["http://{{ influx_Pri_IP }}:{{ influx_Pri_Port }}"]
   database = "{{ influx_priDB_name }}"
   retention_policy = "1hrRP"
   timeout = "5s"
   namepass = ["smsc_log","smsc_cdr","smsc_status","smsc_http_cdr"]
   skip_database_creation = true

